---
title: 'Day 2 Session 1: <br> Static spectral analysis <br> using Linear Mixed-Effect Models'
author: "Takayuki Nagamine"
date: "`r Sys.Date()`"
output: 
  rmdformats::readthedown
    # html_document: 
    # toc: true
    # toc_float: true
    # number_sections: true
---

```{r include=FALSE}
library(rmdformats)
library(tidyverse)

# setting the plot theme globally
theme_set(theme_classic())

# define colour-blind-friendly colour palette 
cbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2",
"#D55E00", "#CC79A7")
```


```{r}
# Load necessary packages
library(lme4)
library(ggplot2)
library(dplyr)

# Set seed for reproducibility
set.seed(123)

# Simulate data
n_participants <- 10
n_items <- 15
n_trials <- 5  # Number of repetitions per condition

participants <- paste0("P", 1:n_participants)
items <- paste0("Item", 1:n_items)
languages <- c("L1", "L2")
vowels <- c("a", "i", "u")

# Expand grid to get all conditions
data <- expand.grid(participant = participants,
                    item = items,
                    language = languages,
                    vowel = vowels,
                    trial = 1:n_trials)

# Generate random intercepts and slopes
by_participant <- data.frame(participant = participants,
                             participant_intercept = rnorm(n_participants, mean = 0, sd = 50),
                             vowel_slope = rnorm(n_participants, mean = 20, sd = 10)) # Individual vowel effects

by_item <- data.frame(item = items,
                      item_intercept = rnorm(n_items, mean = 0, sd = 30),
                      language_slope = rnorm(n_items, mean = -30, sd = 15)) # Item-specific language effect

# Merge random effects
data <- left_join(data, by_participant, by = "participant")
data <- left_join(data, by_item, by = "item")

# Fixed effects: baseline F2, vowel effect, language effect
data$F2 <- 1500 +  # Baseline F2
           ifelse(data$vowel == "i", 100, ifelse(data$vowel == "u", -50, 0)) +  # Vowel effects
           ifelse(data$language == "L2", -80, 0) +  # Language effect
           data$vowel_slope * as.numeric(as.factor(data$vowel)) +  # Random vowel slopes
           data$language_slope * as.numeric(as.factor(data$language)) +  # Random language slopes
           data$participant_intercept +  # Random intercepts per participant
           data$item_intercept +  # Random intercepts per item
           rnorm(nrow(data), mean = 0, sd = 20)  # Residual noise

# Fit mixed-effects model
model <- lmer(F2 ~ language + vowel + language:vowel + 
              (1 + vowel | participant) + (1 + language | item), 
              data = data)

summary(model)

# Plot: Participant-specific vowel slopes
ggplot(data, aes(x = vowel, y = F2, color = participant, group = participant)) +
  geom_point(alpha = 0.6) +
  geom_line(stat = "summary", fun = mean, size = 1) +
  labs(title = "Participant-specific vowel effects on F2", y = "F2 (Hz)", x = "Vowel") +
  facet_wrap(~ vowel)

# Plot: Item-specific language slopes
ggplot(data, aes(x = language, y = F2, color = item, group = item)) +
  geom_point(alpha = 0.6) +
  geom_line(stat = "summary", fun = mean, size = 1) +
  labs(title = "Item-specific language effects on F2", y = "F2 (Hz)", x = "Language")


```


# Statistical analysis

The data visualisation seems to suggest some between-group differences, but it still remains unclear exactly how the two groups differ. Here, let's investigate this further using **linear-mixed effect modelling**. 

Today, we will try to model the formant frequencies (i.e., ```f1z```, ```f2z```, and ```f3z```) as outcome variables as a function of predictor variables, including ```language```, ```vowel``` and the interaction between them (i.e., ```language:vowel```).

We will model the formant frequencies separately for /l/ and /ɹ/ in order to reduce complexity in model output interpretation. This means that for F1 there will be two models (i.e., six models in total for F1, F2 and F3). 

My usual modelling strategy is as follows:

1. I first convert the predictor variables into **factor** variables. At the same time, I check how many levels there are for each predictor variable. Empty levels should be removed at this stage as this will influence the modelling outcomes. 

2. I consider random effect structures. Complex random effects often make the models unable to converge, so I usually spend quite a lot of time adding and removing variables while referring to the summary table. 

3. Once the full model is defined, compare the **full** and **nested** models for the statistical significance check. 

## Preparation 1: Separating the data sets

Let's separate the data sets into two: for /l/ and for /ɹ/.

```{r}
# /l/ model
df_mid_L <- df_mid |> 
  dplyr::filter(segment == "L")

# /ɹ/ model
df_mid_R <- df_mid |> 
  dplyr::filter(segment == "R")
```

## Preparation 2: Converting variables into factor

First, let's convert variable types into **factor**. Here is the code for the /l/ model.

```{r}
# ``language`` variable
df_mid_L$language <- as.factor(df_mid_L$language)
levels(df_mid_L$language)

# ``vowel`` variable
df_mid_L$vowel <- as.factor(df_mid_L$vowel)
levels(df_mid_L$vowel)

# ``speaker`` variable: a random effect
df_mid_L$speaker <- as.factor(df_mid_L$speaker)
levels(df_mid_L$speaker)

# ``word`` variable: a random effect
df_mid_L$word <- as.factor(df_mid_L$word)
levels(df_mid_L$word)

## so far so good, but in case there is any unused level, it can be removed using droplevels()
df_mid_L$word <- droplevels(df_mid_L$word)
```

Similarly, for the /ɹ/ model:

```{r}
# ``language`` variable
df_mid_R$language <- as.factor(df_mid_R$language)
levels(df_mid_L$language)

# ``vowel`` variable
df_mid_R$vowel <- as.factor(df_mid_R$vowel)
levels(df_mid_R$vowel)

# ``speaker`` variable: a random effect
df_mid_R$speaker <- as.factor(df_mid_R$speaker)
levels(df_mid_R$speaker)

# ``word`` variable: a random effect
df_mid_R$word <- as.factor(df_mid_R$word)
levels(df_mid_R$word)

## so far so good, but in case there is any unused level, it can be removed using droplevels()
df_mid_R$word <- droplevels(df_mid_R$word)
```

One important thing to define is the **coding scheme**. Here, I use **treatment coding** because it allows me to understand the model output rather intuitively. 

This means that we need to make sure which level in a given variable serves as a **baseline** level. This can be checked via ```contrasts()``` function as below.

```{r}
# language variable
contrasts(df_mid_L$language)

# vowel variable
contrasts(df_mid_L$vowel)
```

The /ɹ/ data set should have the same coding scheme. These overall suggest that the intercept corresponds to the z-scored formant frequency (along F1, F2 or F3) for L1 English speakers in the /æ/ context. The output table should show a difference from the baseline level. 

## Building models: /l/

Then, let's build a model for F1. We'll begin with the /l/ models.

```{r}
# F1 full model
lm1_L_F1 <- lme4::lmer(f1z ~ 1 + language + vowel + language:vowel + (1 + vowel|speaker) + (1 + language|word), data = df_mid_L, REML = FALSE)
```

We get a **singular fit** warning. This indicates that correlations between random effect variables may not be modelled properly. Let's investigate the model summary to identify the possible causes.

```{r}
summary(lm1_L_F1)$varcor
```

The output above shows that the varying slope/intercept correlation for ```word``` is -1.00. This is usually a red flag indicating some estimation problems in the model. So we'll remove the varying slope for ```word``` depending on the ```language``` variable and rerun the new model below. 

```{r}
# F1 full model
lm2_L_F1 <- lme4::lmer(f1z ~ 1 + language + vowel + language:vowel + (1 + vowel|speaker) + (1|word), data = df_mid_L, REML = FALSE)
```

Now we receive no warning messages, so looking positive. Let's investigate the model output.

```{r}
# random effect correlations
summary(lm2_L_F1)$varcor

# fixed effects
summary(lm2_L_F1)$coefficients
```

Let's spend some time interpreting the fixed effects.

1. **Intercept** shows an estimate of z-normalised F1 values for the baseline level: /l/ for L1 English speakers in the /æ/ condition. This is predicted to be approximately 0.74. 

2. **languageJapanese** shows a difference in F1z values between L1 English (baseline) and L1 Japanese speakers **within the /æ/ context**. This means that, in the /æ/ context, L1 Japanese speakers exhibit the F1 of ca. 0.55 (i.e., 0.74 - 0.19). 

3. **vowel/i/** shows a difference **within L1 English speakers** in F1z values between the /æ/ and /i/ contexts. This means that L1 English speakers produce English /l/ with the z-normalised F1 values of -0.35 in the /i/ context (i.e., 0.74 - 1.09).

4. **vowel/u/** shows a difference, again, within L1 English speakers in F1z values **between the /æ/ and /u/ contexts**. So far, we know that the F1z value for L1 English speakers in the /æ/ context is 0.74: based on this, we can get the F1z value in the /u/ context, which should be approximately -0.65 (i.e., 0.74 - 1.39).

So far, the only occasion when we talked about L1 Japanese speakers was for the **languageJapanese** term. The rest of the two interaction terms get us to obtain F1z values for L1 Japanese speakers in the /i/ and /u/ contexts.

5. In order to obtain the estimated F1z value for L1 Japanese speakers in the /i/ context, we start with the intercept (L1 English speakers in /æ/: **0.74**), then we go onto the /i/ context (still within L1 English speakers: 0.74 **- 1.09**), where we go to L1 Japanese speakers by adding languageJapanese (i.e., 0.74 - 1.09 **- 0.19**). Finally, **languageJapanese:vowel/i/** acts like an adjustment for the interaction term, so that the calculation would be 0.74 - 1.09 - 0.19 + **0.09** = -0.45.

6. Similarly, let's do the same to get estimated F1z values for L1 Japanese speakers in the /u/ context. Starting from the baseline (0.74), then going onto the /u/ context (i.e., 0.74 **- 1.39**), then onto Japanese (i.e., 0.74 - 1.39 **- 0.19**), and finally the **languageJapanese:vowel/u/** adjustment of - 0.01, resulting in 0.74 - 1.39 - 0.19 - **0.01** = -0.85.

Let's compare these estimated values with the raw data. The model estimates can be obtained using the ```emmeans::emmeans()``` function.

```{r}
# raw data
df_mid_L |> 
  dplyr::group_by(language, vowel) |> 
  dplyr::summarise(
    mean_f1z = mean(f1z)
  ) |> 
  dplyr::ungroup()

# estimated marginal means from the model
emmeans::emmeans(lm2_L_F1, ~ vowel | language)
```

The raw data do not always match the estimated values as there are some other factors that are not incorporated in the modelling. But overall, the estimated values are close to the raw values so we should be good to go. 

## Assumption check

Statistical models usually rely on assumptions. Let's check whether the linear model we've just fit meets the **normality** and **constant variance** assumptions.

When the model meets the normality assumption, the **residuals** are usually normally distributed. The residual is the error between an observed data value and a predicted value. Similarly, when the model satisfies the constant variance assumption (i.e., **heteroscedasticity**), the spread of residuals should be constant along the regression line.

The first plot shows the histgram of residuals, and it seems to show that the residuals are normally distributed. The plot in the middle is called a **quantile-quantile (Q-Q) plot**, in which the residuals show a (nearly) linear relationships when satisfying the residual assumption. Finally, the **residual plot** on the right should show that the spread of the residuals is approximately equal across the range of fitted values without any particular linear trend. 

The plots below suggest that our model ```lm2_L_F1``` meets all these assumptions. So far, so good. 

```{r}
par(mfrow = c(1, 3))

# histogram
hist(residuals(lm2_L_F1), col = "skyblue2")

# Q-Q plot
qqnorm(residuals(lm2_L_F1))
qqline(residuals(lm2_L_F1))

# residual plot
plot(fitted(lm2_L_F1), residuals(lm2_L_F1))
```

## Significance testing through likelihood ratio tests (LRTs)

Now that we better understand the model, let's check statistical significance of the fixed effects. A recommended approach is through model comparison via likelihood ratio tests (LRTs) using ```anova()``` function. Model comparison requires two models: a model with the effect of interested included and another (reduced/nested) model that excludes the effect to be tested. 

Note also that, for model comparisons, each model needs to be estimated using **maximal likelihood (ML) estimation**. This corresponds to the ```REML = FALSE``` argument in the model. 

### Interaction between ```language``` and ```vowel```

First, let's test whether the interaction between ```language``` and ```vowel``` is statistically significant. For this, we compare two models: (1) a full model that contains the interaction term and (2) a reduced model without the interaction term. 

```{r}
# F1 full model
lm2_L_F1 <- lme4::lmer(f1z ~ 1 + language + vowel + language:vowel + (1 + vowel|speaker) + (1|word), data = df_mid_L, REML = FALSE)

# F1 nested model -- without interaction
lm3_L_F1 <- lme4::lmer(f1z ~ 1 + language + vowel + (1 + vowel|speaker) + (1|word), data = df_mid_L, REML = FALSE)

# model comparison
anova(lm2_L_F1, lm3_L_F1, test = "Chisq")
```

Model comparison computes a *p* value, showing whether a fuller model improves the degree of model fit at statistically significant level. The output above shows a *p* value above 0.05, meaning that there is little evidence that the two models just compared here differ in the degree of model fit. In other words, the interaction between ```language``` and ```vowel``` does not contribute much to improve the model. 

In this case, it is generally preferred to select more **parsimonious** (i.e., simpler) model because it is more economical in the sense that fewer predictors achieve the same (or even better) degree of model fits. This means that there are now two fixed effects to test: ```language``` and ```vowel```. 

### Main effect of ```language```

Let's check whether ```language``` has an overall effect on the F1z values. 

```{r}
# F1 nested model -- without interaction
lm3_L_F1 <- lme4::lmer(f1z ~ 1 + language + vowel + (1 + vowel|speaker) + (1|word), data = df_mid_L, REML = FALSE)

# F1 nested model -- without language
lm4_L_F1 <- lme4::lmer(f1z ~ 1 + vowel + (1 + vowel|speaker) + (1|word), data = df_mid_L, REML = FALSE)

# model comparison
anova(lm3_L_F1, lm4_L_F1, test = "Chisq")
```

The model comparison output suggests that ```lm3_L_F1``` (i.e., fuller model) has a better model fit at statistically significant level at *p* = 0.03. This means that the effect of ```language``` has an overall effect on the F1z values at statistically significant level.

### Main effect of ```vowel```

How about the effect of ```vowel```? Let's check this here:

```{r}
# F1 nested model -- without interaction
lm3_L_F1 <- lme4::lmer(f1z ~ 1 + language + vowel + (1 + vowel|speaker) + (1|word), data = df_mid_L, REML = FALSE)

# F1 nested model -- without vowel
lm5_L_F1 <- lme4::lmer(f1z ~ 1 + language + (1|speaker) + (1|word), data = df_mid_L, REML = FALSE)

# model comparison
anova(lm3_L_F1, lm5_L_F1, test = "Chisq")
```

The model output still suggests that ```lm3_L_F1``` (i.e., fuller model) shows an improved model fit at statistically significant level of *p* < 0.001.

## Post-hoc analysis

Now that we know that ```language``` and ```vowel``` have a statistically significant effect on F1, let's further investigate where exactly the statistical siginificance is identified. The LRTs tell us which variable has an statistically significant effect, it does not tell us which levels. 

For this, we can use the ```emmeans::emmeans()``` function for pairwise comparison. Although the ```language``` term is fairly obvious given there are only two levels, let's try this anyway to see what the output looks like. 

```{r}
# language effect
emmeans::emmeans(lm3_L_F1, pairwise ~ language, adjust = "tukey")

# vowel effect
emmeans::emmeans(lm3_L_F1, pairwise ~ vowel, adjust = "tukey")
```

For the ```language``` effect, L1 English and L1 Japanese speakers differ very obviously. 

For the ```vowel``` effect, it turns out that statistically siginificant differences are identified between (1) /æ/ and /i/ and (2) /æ/ - /u/, but not between /i/ and /u/. This suggests that it is /æ/ that drives the statistical significance of the ```vowel``` effect, which makes sense as it is the only low/open vowel for which F1 tends to be higher. 

# Summary: F1 for English /l/

- The best fit model:

  - ```f1z ~ 1 + language + vowel + (1 + vowel | speaker) + (1 | word)```

- Both ```language``` and ```vowel``` have an overall effect on the z-normalised F1 values for English /l/. 

  - The ```language``` effect obviously results from the L1 English - L1 Japanese speakers difference.
  
  - The ```vowel``` effect seems to be driven by /æ/ that shows higher F1 than /i/ or /u/.

# Statistics: Your turn!

## Modelling other parameters

So far, we have modelled F1 values for English /l/. How about F2 and F3? How about English /ɹ/? Follow the procedure commented below and complete an analysis of a parameter that you're interested in. 

To reiterate: 

- the data set for English /l/ is ```df_mid_L```

- the data set for English /ɹ/ is ```df_mid_R```

### Check the column names

```{r}
# check the column names
# ...
```

### Convert each variable into factor

```{r}
# convert ``language`` variable into factor variable
# ...

# ``vowel`` variable 
# ...
# ...

# ``speaker`` variable: a random effect
# ...
# ...

# ``word`` variable: a random effect
# ...
# ...

## so far so good, but in case there is any unused level, it can be removed using droplevels()
## try it with the ```word``` variable
# ...
```

### Check the baseline level 

Our modelling is based on the treatment coding scheme, so let's check the baseline level. 

```{r}
# language variable
# ...

# vowel variable
# ...
```

### Building a full model

```{r}
# ...
```

### Assumption check

```{r}
# ...
```

### Significance testing

```{r}
# ...
```


## Advanced: modelling English /l/ and /ɹ/ together

We have decided to build separate models for English /l/ and /ɹ/. But it is also possible to model both together. How would you do it? How do such differences in modelling influence the way you formulate research questions?

# Wrap-up question

Let's wrap up this session by discussing what research questions you could possibly ask based on the static analysis using linear mixed-effect models. Does it fully explain the mechanism underlying the Japanese /r/-/l/ problem? Do you struggle to disentangle something from something else? 

# Session information

```{r}
sessionInfo()
```

```{r include=FALSE}
save(df_mid, file = "data/df_mid.rda")
```

