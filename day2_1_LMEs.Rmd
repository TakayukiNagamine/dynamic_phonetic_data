---
title: 'Day 2 Session 1: <br> Static spectral analysis <br> using Linear Mixed-Effect Models'
author: "Takayuki Nagamine"
date: "`r Sys.Date()`"
output: 
  rmdformats::readthedown
    # html_document: 
    # toc: true
    # toc_float: true
    # number_sections: true
---

```{r include=FALSE}
library(rmdformats)
library(tidyverse)
library(kableExtra)  # For nice tables
library(knitr)

# setting the plot theme globally
theme_set(theme_classic())

# define colour-blind-friendly colour palette 
cbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2",
"#D55E00", "#CC79A7")
```

# Introduction

On Day 1, we tried **bottom-up**, **data-driven** approach to spectral analysis using **principal component analysis (PCA)** and **functional principal component analysis (FPCA)**. They were all data-driven approaches to **hypothesis generation**, although it was also possible to conduct hypothesis testing using the derived PC scores. PCA and FPCA offer flexible approach to data analysis given that you can analyse both static and dynamic analyses with statistical tests that you're familiar with. 

Through PCA and FPCA, we also explored the data and found out that **the second and third formants** can be an important acoustic parameter to better understand the underlying mechanism in L1 Japanese speakers' difficulty in producing English /l/ and /ɹ/. From this, we could formulate a (general) hypothesis that:

```
L1 Japanese speakers show differences in F2 and F3 characteristics in their production of L2 English /l/ and /ɹ/ compared to L1 English speakers.
```

Let's test the hypothesis using two methods today: **linear mixed-effect models (LMEs)** for static analysis and **generalised additive mixed-effect models (GAMMs)** for dynamic analysis. 

# LMEs and GAMMs: Overview

Here are some key similarities and differences between LMEs and GAMMs:

```{r echo=FALSE, message=FALSE, warning=FALSE}
# R Markdown table using knitr::kable
table_data <- data.frame(
  Feature = c("Model Type", "Handling of Nonlinearity", "Use in Acoustic Phonetics", 
              "Fixed Effects", "Random Effects", "Smooth Terms", "Interpretability", 
              "Computational Complexity"),
  LMEs = c("Linear regression with mixed effects", 
          "Assumes linear relationships", 
          "Used for analyzing static formant values, VOT, speech rate, etc.", 
          "Effects modeled as linear coefficients", 
          "Accounts for speaker and item variability", 
          "Not applicable", 
          "Easy to interpret fixed effects", 
          "Generally lower"),
  GAMMs = c("Generalized additive model with mixed effects", 
           "Can model nonlinear relationships using splines", 
           "Used for time-varying speech data like formant trajectories, f0 contours", 
           "Effects can be linear or smooth", 
           "Accounts for speaker and item variability", 
           "Uses splines to model smooth variation over time", 
           "More complex due to smooth terms", 
           "Higher due to spline fitting")
)

# Display as a nice HTML table
kable(table_data, format = "html", escape = FALSE)  |> 
  kable_styling("striped", full_width = F)
```

Linear mixed-effect models are potent tools in phonetics research nowadays for its capability to account for **random effects**. They usually refer to the variation in the data caused by factors that we are not interested in but still cannot ignore really. In phonetics research, this is usually associated with the effects of **speaker/participant** and **item/word**. 

For example, the following visualisation exemplifies speaker- and item-related random effects. The first plot shows that each speaker gets their own intercept (= **random intercept**) and also different degrees of slope for each vowel context (= **random slope**). This is notated in the LMEs as: ```(1 + vowel | participant)```.

Similarly, in the second plot, it is shown that each item gets their own intercept (= **random intercept**) and also different slops depending on the speaker's L1 (= **random slope**). This can be specified in the model as ```(1 + language | item)```.

```{r echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
# Load necessary packages
library(lme4)
library(ggplot2)
library(dplyr)

# Set seed for reproducibility
set.seed(123)

# Simulate data
n_participants <- 10
n_items <- 15
n_trials <- 5  # Number of repetitions per condition

participants <- paste0("P", 1:n_participants)
items <- paste0("Item", 1:n_items)
languages <- c("L1", "L2")
vowels <- c("a", "i", "u")

# Expand grid to get all conditions
data <- expand.grid(participant = participants,
                    item = items,
                    language = languages,
                    vowel = vowels,
                    trial = 1:n_trials)

# Generate random intercepts and slopes
by_participant <- data.frame(participant = participants,
                             participant_intercept = rnorm(n_participants, mean = 0, sd = 50),
                             vowel_slope = rnorm(n_participants, mean = 20, sd = 10)) # Individual vowel effects

by_item <- data.frame(item = items,
                      item_intercept = rnorm(n_items, mean = 0, sd = 30),
                      language_slope = rnorm(n_items, mean = -30, sd = 15)) # Item-specific language effect

# Merge random effects
data <- left_join(data, by_participant, by = "participant")
data <- left_join(data, by_item, by = "item")

# Fixed effects: baseline F2, vowel effect, language effect
data$F2 <- 1500 +  # Baseline F2
           ifelse(data$vowel == "i", 100, ifelse(data$vowel == "u", -50, 0)) +  # Vowel effects
           ifelse(data$language == "L2", -80, 0) +  # Language effect
           data$vowel_slope * as.numeric(as.factor(data$vowel)) +  # Random vowel slopes
           data$language_slope * as.numeric(as.factor(data$language)) +  # Random language slopes
           data$participant_intercept +  # Random intercepts per participant
           data$item_intercept +  # Random intercepts per item
           rnorm(nrow(data), mean = 0, sd = 20)  # Residual noise

# Fit mixed-effects model
model <- lmer(F2 ~ language + vowel + language:vowel + 
              (1 + vowel | participant) + (1 + language | item), 
              data = data)

summary(model)

# Plot: Participant-specific vowel slopes
ggplot(data, aes(x = vowel, y = F2, color = participant, group = participant)) +
  geom_point(alpha = 0.6) +
  geom_line(stat = "summary", fun = mean, size = 1) +
  labs(title = "Participant-specific vowel effects on F2", y = "F2 (Hz)", x = "Vowel")

# Plot: Item-specific language slopes
ggplot(data, aes(x = language, y = F2, color = item, group = item)) +
  geom_point(alpha = 0.6) +
  geom_line(stat = "summary", fun = mean, size = 1) +
  labs(title = "Item-specific language effects on F2", y = "F2 (Hz)", x = "Language")
```

For future reference, **GAMMs** can also account for random effects. Later today, we'll use an approach based on **factor smooths**, in which each participant/item gets its own time-varying smooths. This can be thought of as a random effect combining both intercept and slope.

```{r echo=FALSE, message=FALSE, warning=FALSE, results=FALSE}
library(mgcv)
library(ggplot2)
library(dplyr)

# Simulated data
set.seed(123)
n_speakers <- 5
n_timepoints <- 50

data <- expand.grid(Speaker = paste0("S", 1:n_speakers), Time = seq(0, 1, length.out = n_timepoints))
data$F2 <- with(data, 1500 + 300*sin(2*pi*Time) + rnorm(nrow(data), 0, 50) + as.numeric(Speaker) * 20)

# Fit a GAMM with factor smooths
mod <- gam(F2 ~ s(Time, by = Speaker) + s(Speaker, bs="re"), data = data, method = "REML")

# Predict for visualization
pred <- data |> 
  group_by(Speaker, Time) |> 
  summarise(F2_fit = predict(mod, newdata = data.frame(Speaker = unique(Speaker), Time = Time), type = "response"), .groups = "drop")

# Plot the factor smooths
ggplot(pred, aes(x = Time, y = F2_fit, color = Speaker)) +
  geom_line(size = 1.2) +
  labs(title = "Factor Smooths in GAMMs",
       x = "Time (normalized)",
       y = "Predicted F2 (Hz)",
       color = "Speaker")
```

# Data preparation: Preliminaries

We'll first do some preliminary process running up to the LMEs analysis. 

## Loading packages

```{r message=FALSE, warning=FALSE}
library(tidyverse) # data wrangling and visualisation
library(lme4) # for fitting models
library(lmerTest) # for obtaining details in the lme4 results
library(emmeans) # for post-hoc analysis
```

## Loading data

```{r message=FALSE}
# import the csv file "initial.liquid.static.csv" from the "data" directory and save it as "df_mid"
df_mid <- readr::read_csv("data/initial.liquid.static.csv")
```

# Check data & data wrangling

As usual, let's start with checking the data.

```{r}
# check the columns
colnames(df_mid)
```

We'll omit some variables and rename some variable names as we did yesterday.

```{r}
# remove columns 
df_mid <- df_mid |> 
  dplyr::select(-c(IsApproximant, IsAcoustic, omit))

# convert the ARPABET notations into the IPA symbols.
df_mid <- df_mid |> 
  dplyr::mutate(
    vowel = case_when(
      next_sound == "AE1" ~ "/æ/",
      next_sound == "IY1" ~ "/i/",
      next_sound == "UW1" ~ "/u/",
    )
  )
```

## Checking the number of participants, tokens...

Let's also obtain some descriptive statistics here.

```{r}
# number of participants
df_mid |> 
  dplyr::group_by(language) |> 
  dplyr::summarise(n = n_distinct(speaker)) |> 
  dplyr::ungroup()

# number of tokens per segment
df_mid |> 
  dplyr::group_by(segment) |> 
  dplyr::summarise(n = n()) |> 
  dplyr::ungroup()
```

## Scaling formant values

To compare formant values across participants, we'll normalise them using z-scores.

```{r}
df_mid <- df_mid |> 
  dplyr::group_by(speaker) |> # tell R to do the following iteration per speaker
  dplyr::mutate(
    f1z = as.numeric(scale(f1)), # scale f1 into z-score
    f2z = as.numeric(scale(f2)), # scale f2 into z-score
    f3z = as.numeric(scale(f3)) # scale f3 into z-score
  ) |> 
  dplyr::ungroup() # don't forget ungrouping
```

And check the mean and sd:

```{r}
# check mean and sd of raw/scaled F1 values for each speaker
df_mid |> 
  dplyr::group_by(speaker) |>
  dplyr::summarise(
    f1_mean = mean(f1),
    f1_sd = sd(f1),
    f1z_mean = mean(f1z),
    f1z_sd = sd(f1z)
  ) |> 
  dplyr::ungroup() 
```

# Data visualisation

### Your turn

Could you visualise F1, F2 and F3 values using ```ggpplot```? If you are not sure how to do so, please refer back to the Day 1 PCA materials.

## F1

```{r warning=FALSE, message=FALSE}
# df_mid |> 
# ...
```

## F2

```{r warning=FALSE, message=FALSE}
# df_mid |> 
# ...
```

## F3


```{r warning=FALSE, message=FALSE}
# df_mid |> 
# ...
```

# Linear mixed-effect models (LMEs)

The data visualisation seems to suggest some between-group differences, but it still remains unclear exactly how the two groups differ. Here, let's investigate this further using **linear-mixed effect modelling**. 

Today, we will try to model the formant frequencies (i.e., ```f1z```, ```f2z```, and ```f3z```) as outcome variables as a function of predictor variables, including ```language```, ```vowel``` and the interaction between them (i.e., ```language:vowel```).

We will model the formant frequencies separately for /l/ and /ɹ/ in order to reduce complexity in model output interpretation. This means that for F1 there will be two models (i.e., six models in total for F1, F2 and F3). 

My usual modelling strategy is as follows:

1. I first convert the predictor variables into **factor** variables. At the same time, I check how many levels there are for each predictor variable. Empty levels should be removed at this stage as this will influence the modelling outcomes. 

2. I consider random effect structures. Complex random effects often make the models unable to converge, so I usually spend quite a lot of time adding and removing variables while referring to the summary table. 

3. Once the full model is defined, compare the **full** and **nested** models for the statistical significance check. 

## Preparation 1: Separating the data sets

Let's separate the data sets into two: for /l/ and for /ɹ/.

```{r}
# /l/ model
df_mid_L <- df_mid |> 
  dplyr::filter(segment == "L")

# /ɹ/ model
df_mid_R <- df_mid |> 
  dplyr::filter(segment == "R")
```

## Preparation 2: Converting variables into factor

First, let's convert variable types into **factor**. Here is the code for the /l/ model.

```{r}
# ``language`` variable
df_mid_L$language <- as.factor(df_mid_L$language)
levels(df_mid_L$language)

# ``vowel`` variable
df_mid_L$vowel <- as.factor(df_mid_L$vowel)
levels(df_mid_L$vowel)

# ``speaker`` variable: a random effect
df_mid_L$speaker <- as.factor(df_mid_L$speaker)
levels(df_mid_L$speaker)

# ``word`` variable: a random effect
df_mid_L$word <- as.factor(df_mid_L$word)
levels(df_mid_L$word)

## so far so good, but in case there is any unused level, it can be removed using droplevels()
df_mid_L$word <- droplevels(df_mid_L$word)
```

Similarly, for the /ɹ/ model:

```{r}
# ``language`` variable
df_mid_R$language <- as.factor(df_mid_R$language)
levels(df_mid_L$language)

# ``vowel`` variable
df_mid_R$vowel <- as.factor(df_mid_R$vowel)
levels(df_mid_R$vowel)

# ``speaker`` variable: a random effect
df_mid_R$speaker <- as.factor(df_mid_R$speaker)
levels(df_mid_R$speaker)

# ``word`` variable: a random effect
df_mid_R$word <- as.factor(df_mid_R$word)
levels(df_mid_R$word)

## so far so good, but in case there is any unused level, it can be removed using droplevels()
df_mid_R$word <- droplevels(df_mid_R$word)
```

One important thing to define is the **coding scheme**. Here, I use **treatment coding** because it allows me to understand the model output rather intuitively. 

This means that we need to make sure which level in a given variable serves as a **baseline** level. This can be checked via ```contrasts()``` function as below.

```{r}
# language variable
contrasts(df_mid_L$language)

# vowel variable
contrasts(df_mid_L$vowel)
```

The /ɹ/ data set should have the same coding scheme. These overall suggest that the intercept corresponds to the z-scored formant frequency (along F1, F2 or F3) for L1 English speakers in the /æ/ context. The output table should show a difference from the baseline level. 

## Building models: /l/

Then, let's build a model for F1. We'll begin with the /l/ models.

```{r}
# F1 full model
lm1_L_F1 <- lme4::lmer(f1z ~ 1 + language + vowel + language:vowel + (1 + vowel|speaker) + (1 + language|word), data = df_mid_L, REML = FALSE)
```

We get a **singular fit** warning. This indicates that correlations between random effect variables may not be modelled properly. Let's investigate the model summary to identify the possible causes.

```{r}
summary(lm1_L_F1)$varcor
```

The output above shows that the varying slope/intercept correlation for ```word``` is -1.00. This is usually a red flag indicating some estimation problems in the model. So we'll remove the varying slope for ```word``` depending on the ```language``` variable and rerun the new model below. 

```{r}
# F1 full model
lm2_L_F1 <- lme4::lmer(f1z ~ 1 + language + vowel + language:vowel + (1 + vowel|speaker) + (1|word), data = df_mid_L, REML = FALSE)
```

Now we receive no warning messages, so looking positive. Let's investigate the model output.

```{r}
# random effect correlations
summary(lm2_L_F1)$varcor

# fixed effects
summary(lm2_L_F1)$coefficients
```

Let's spend some time interpreting the fixed effects.

1. **Intercept** shows an estimate of z-normalised F1 values for the baseline level: /l/ for L1 English speakers in the /æ/ condition. This is predicted to be approximately 0.74. 

2. **languageJapanese** shows a difference in F1z values between L1 English (baseline) and L1 Japanese speakers **within the /æ/ context**. This means that, in the /æ/ context, L1 Japanese speakers exhibit the F1 of ca. 0.55 (i.e., 0.74 - 0.19). 

3. **vowel/i/** shows a difference **within L1 English speakers** in F1z values between the /æ/ and /i/ contexts. This means that L1 English speakers produce English /l/ with the z-normalised F1 values of -0.35 in the /i/ context (i.e., 0.74 - 1.09).

4. **vowel/u/** shows a difference, again, within L1 English speakers in F1z values **between the /æ/ and /u/ contexts**. So far, we know that the F1z value for L1 English speakers in the /æ/ context is 0.74: based on this, we can get the F1z value in the /u/ context, which should be approximately -0.65 (i.e., 0.74 - 1.39).

So far, the only occasion when we talked about L1 Japanese speakers was for the **languageJapanese** term. The rest of the two interaction terms get us to obtain F1z values for L1 Japanese speakers in the /i/ and /u/ contexts.

5. In order to obtain the estimated F1z value for L1 Japanese speakers in the /i/ context, we start with the intercept (L1 English speakers in /æ/: **0.74**), then we go onto the /i/ context (still within L1 English speakers: 0.74 **- 1.09**), where we go to L1 Japanese speakers by adding languageJapanese (i.e., 0.74 - 1.09 **- 0.19**). Finally, **languageJapanese:vowel/i/** acts like an adjustment for the interaction term, so that the calculation would be 0.74 - 1.09 - 0.19 + **0.09** = -0.45.

6. Similarly, let's do the same to get estimated F1z values for L1 Japanese speakers in the /u/ context. Starting from the baseline (0.74), then going onto the /u/ context (i.e., 0.74 **- 1.39**), then onto Japanese (i.e., 0.74 - 1.39 **- 0.19**), and finally the **languageJapanese:vowel/u/** adjustment of - 0.01, resulting in 0.74 - 1.39 - 0.19 - **0.01** = -0.85.

Let's compare these estimated values with the raw data. The model estimates can be obtained using the ```emmeans::emmeans()``` function.

```{r}
# raw data
df_mid_L |> 
  dplyr::group_by(language, vowel) |> 
  dplyr::summarise(
    mean_f1z = mean(f1z)
  ) |> 
  dplyr::ungroup()

# estimated marginal means from the model
emmeans::emmeans(lm2_L_F1, ~ vowel | language)
```

The raw data do not always match the estimated values as there are some other factors that are not incorporated in the modelling. But overall, the estimated values are close to the raw values so we should be good to go. 

## Assumption check

Statistical models usually rely on assumptions. Let's check whether the linear model we've just fit meets the **normality** and **constant variance** assumptions.

When the model meets the normality assumption, the **residuals** are usually normally distributed. The residual is the error between an observed data value and a predicted value. Similarly, when the model satisfies the constant variance assumption (i.e., **heteroscedasticity**), the spread of residuals should be constant along the regression line.

The first plot shows the histgram of residuals, and it seems to show that the residuals are normally distributed. The plot in the middle is called a **quantile-quantile (Q-Q) plot**, in which the residuals show a (nearly) linear relationships when satisfying the residual assumption. Finally, the **residual plot** on the right should show that the spread of the residuals is approximately equal across the range of fitted values without any particular linear trend. 

The plots below suggest that our model ```lm2_L_F1``` meets all these assumptions. So far, so good. 

```{r}
par(mfrow = c(1, 3))

# histogram
hist(residuals(lm2_L_F1), col = "skyblue2")

# Q-Q plot
qqnorm(residuals(lm2_L_F1))
qqline(residuals(lm2_L_F1))

# residual plot
plot(fitted(lm2_L_F1), residuals(lm2_L_F1))
```

## Significance testing through likelihood ratio tests (LRTs)

Now that we better understand the model, let's check statistical significance of the fixed effects. A recommended approach is through model comparison via likelihood ratio tests (LRTs) using ```anova()``` function. Model comparison requires two models: a model with the effect of interested included and another (reduced/nested) model that excludes the effect to be tested. 

Note also that, for model comparisons, each model needs to be estimated using **maximal likelihood (ML) estimation**. This corresponds to the ```REML = FALSE``` argument in the model. 

### Interaction between ```language``` and ```vowel```

First, let's test whether the interaction between ```language``` and ```vowel``` is statistically significant. For this, we compare two models: (1) a full model that contains the interaction term and (2) a reduced model without the interaction term. 

```{r}
# F1 full model
lm2_L_F1 <- lme4::lmer(f1z ~ 1 + language + vowel + language:vowel + (1 + vowel|speaker) + (1|word), data = df_mid_L, REML = FALSE)

# F1 nested model -- without interaction
lm3_L_F1 <- lme4::lmer(f1z ~ 1 + language + vowel + (1 + vowel|speaker) + (1|word), data = df_mid_L, REML = FALSE)

# model comparison
anova(lm2_L_F1, lm3_L_F1, test = "Chisq")
```

Model comparison computes a *p* value, showing whether a fuller model improves the degree of model fit at statistically significant level. The output above shows a *p* value above 0.05, meaning that there is little evidence that the two models just compared here differ in the degree of model fit. In other words, the interaction between ```language``` and ```vowel``` does not contribute much to improve the model. 

In this case, it is generally preferred to select more **parsimonious** (i.e., simpler) model because it is more economical in the sense that fewer predictors achieve the same (or even better) degree of model fits. This means that there are now two fixed effects to test: ```language``` and ```vowel```. 

### Main effect of ```language```

Let's check whether ```language``` has an overall effect on the F1z values. 

```{r}
# F1 nested model -- without interaction
lm3_L_F1 <- lme4::lmer(f1z ~ 1 + language + vowel + (1 + vowel|speaker) + (1|word), data = df_mid_L, REML = FALSE)

# F1 nested model -- without language
lm4_L_F1 <- lme4::lmer(f1z ~ 1 + vowel + (1 + vowel|speaker) + (1|word), data = df_mid_L, REML = FALSE)

# model comparison
anova(lm3_L_F1, lm4_L_F1, test = "Chisq")
```

The model comparison output suggests that ```lm3_L_F1``` (i.e., fuller model) has a better model fit at statistically significant level at *p* = 0.03. This means that the effect of ```language``` has an overall effect on the F1z values at statistically significant level.

### Main effect of ```vowel```

How about the effect of ```vowel```? Let's check this here:

```{r}
# F1 nested model -- without interaction
lm3_L_F1 <- lme4::lmer(f1z ~ 1 + language + vowel + (1 + vowel|speaker) + (1|word), data = df_mid_L, REML = FALSE)

# F1 nested model -- without vowel
lm5_L_F1 <- lme4::lmer(f1z ~ 1 + language + (1|speaker) + (1|word), data = df_mid_L, REML = FALSE)

# model comparison
anova(lm3_L_F1, lm5_L_F1, test = "Chisq")
```

The model output still suggests that ```lm3_L_F1``` (i.e., fuller model) shows an improved model fit at statistically significant level of *p* < 0.001.

## Post-hoc analysis

Now that we know that ```language``` and ```vowel``` have a statistically significant effect on F1, let's further investigate where exactly the statistical siginificance is identified. The LRTs tell us which variable has an statistically significant effect, it does not tell us which levels. 

For this, we can use the ```emmeans::emmeans()``` function for pairwise comparison. Although the ```language``` term is fairly obvious given there are only two levels, let's try this anyway to see what the output looks like. 

```{r}
# language effect
emmeans::emmeans(lm3_L_F1, pairwise ~ language, adjust = "tukey")

# vowel effect
emmeans::emmeans(lm3_L_F1, pairwise ~ vowel, adjust = "tukey")
```

For the ```language``` effect, L1 English and L1 Japanese speakers differ very obviously. 

For the ```vowel``` effect, it turns out that statistically siginificant differences are identified between (1) /æ/ and /i/ and (2) /æ/ - /u/, but not between /i/ and /u/. This suggests that it is /æ/ that drives the statistical significance of the ```vowel``` effect, which makes sense as it is the only low/open vowel for which F1 tends to be higher. 

# Summary: F1 for English /l/

- The best fit model:

  - ```f1z ~ 1 + language + vowel + (1 + vowel | speaker) + (1 | word)```

- Both ```language``` and ```vowel``` have an overall effect on the z-normalised F1 values for English /l/. 

  - The ```language``` effect obviously results from the L1 English - L1 Japanese speakers difference.
  
  - The ```vowel``` effect seems to be driven by /æ/ that shows higher F1 than /i/ or /u/.

# Statistics: Your turn!

## Modelling other parameters

So far, we have modelled F1 values for English /l/. How about F2 and F3? How about English /ɹ/? Follow the procedure commented below and complete an analysis of a parameter that you're interested in. 

To reiterate: 

- the data set for English /l/ is ```df_mid_L```

- the data set for English /ɹ/ is ```df_mid_R```

### Check the column names

```{r}
# check the column names
# ...
```

### Convert each variable into factor

```{r}
# convert ``language`` variable into factor variable
# ...

# ``vowel`` variable 
# ...
# ...

# ``speaker`` variable: a random effect
# ...
# ...

# ``word`` variable: a random effect
# ...
# ...

## so far so good, but in case there is any unused level, it can be removed using droplevels()
## try it with the ```word``` variable
# ...
```

### Check the baseline level 

Our modelling is based on the treatment coding scheme, so let's check the baseline level. 

```{r}
# language variable
# ...

# vowel variable
# ...
```

### Building a full model

```{r}
# ...
```

### Assumption check

```{r}
# ...
```

### Significance testing

```{r}
# ...
```


## Advanced: modelling English /l/ and /ɹ/ together

We have decided to build separate models for English /l/ and /ɹ/. But it is also possible to model both together. How would you do it? How do such differences in modelling influence the way you formulate research questions?

# Wrap-up question

Let's wrap up this session by discussing what research questions you could possibly ask based on the static analysis using linear mixed-effect models. Does it fully explain the mechanism underlying the Japanese /r/-/l/ problem? Do you struggle to disentangle something from something else? 

# Session information

```{r}
sessionInfo()
```

```{r include=FALSE}
save(df_mid, file = "data/df_mid.rda")
```
