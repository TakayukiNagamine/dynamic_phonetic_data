---
title: 'Day 1 Session 1: <br> Static spectral analysis <br> using linear mixed-effect modelling'
author: "Takayuki Nagamine"
date: "`r Sys.Date()`"
output: 
  rmdformats::readthedown
    # html_document: 
    # toc: true
    # toc_float: true
    # number_sections: true
---

```{r include=FALSE}
library(rmdformats)
library(tidyverse)

# setting the plot theme globally
theme_set(theme_classic())

# define colour-blind-friendly colour palette 
cbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2",
"#D55E00", "#CC79A7")
```


# Introduction

Welcome to the workshop **Analysing contours in phonetic data: Dynamic analysis hands-on workshop using R**! In this workshop, we are going to study two main approachces to dynamic data analysis that are often used in phonetics.

The overarching aim of this workshop is to enable you to attempt different approaches to analysing phonetic data. Specifically, I aim to showcase the following contrasts in data analysis methods:

## 1. **static** vs **dynamic** data analysis

Static analysis is a common approach to formant analysis in phonetics research. It is often the case, for example, that we extract formant frequencies during a vocalic interval at a single point in time (e.g., segmental onset, temporal midpoint). This is a farily prevalent approach and it is useful when we try to understand general and abstract properties of segments.

While static analysis is a somewhat easy and computationally less expensive approach, this inevitably involves some degrees of data loss/compression. Recent studies have increasingly demonstrated that **time-varying** properties in speech signals provide rather fine-grained phonetic details that may be important to better understand e.g., synchronic and diachronic variation and change, and cross-linguistic differences/influences in the acquisition of phonetics/phonology. And this is where dynamic analysis methods will come in handy, in which we directly model non-linearity in the data without having to compress/losing much information. My motto in dynamic analysis is **modelling what you see**.

## 2. **top-down** vs **bottom-up** approaches to statistical modelling

Another contrast that will be discussed in this workshop is the difference in the way we generate research hypothesis. Statistical modelling usually starts with formulating hypothesis and research questions while identifying key variables of interest, mainly through reviewing previous literature. In a way, this involves a **top-down** decision making, and hypothesis testing involves asking whether the key variables of interest have a (statistically) significant effect in distinguishing two different populations. 

On the other hand, we may also be interested in seeing what the data show us without any prior knowledge. In other words, our study can be highly exploratory, and we sometimes prefer **data-driven** (or **bottom-up**) approach to identifying key variables of interest, which then feeds hypothesis/research question generations. The contrast between top-down and bottom-up approaches to data analysis influence the nature of research questions that can be asked using each method. 

## Summary

In the next two days, we will be mainly discussing four statistical methods corresponding to the contrasts above.

1. **Linear Mixed-Effect Models** (LMEs)

  - **static** and **top-down** approach


2. **Principal Component Analysis** (PCA) 

  - **static** and **bottom-up** approach


3. **Generalised Additive Mixed-Effect Models** (GAMMs) 

  - **dynamic** and **top-down** approach


4. **Functional Principal Component Analysis** (FPCA)

  - **dynamic** and **bottom-up** approach


Note that I do not wish to advocate one method over another: the main objective of this workshop is for us to experience that the same sets of data can be analysed differently, that data visualisation can look different, and that we can ask different research questions from the same sets of data. 

In the next section, I will introduce a problem that we will be working on in the course of the workshop. Given the nature of my expertise, I only manage to contexualise the workshop on a somewhat specific research topic, but hopefully the workshop offers something exciting for everyone! 

# Contexualising the workshop

## L1 Japanese speakers' production of L2 English /l/ and /ɹ/

I contexualise this workshop broadly in the second language (L2) speech learning. More specifically, my research addresses the ''Japanese /r/-/l/ problem'' (Flege, Aoyama & Bohn, 2021, p. 84). There is a long history of research on this topic, in which the first well-known, influential empirical studies could date back to 1970's (e.g., Goto et al., 1971; Miyawaki et al., 1975). Up until today, lots of researchers have worked on this topic to explain the substantial difficulty that L1 Japanese speakers have in perceiving and producing L2 English liquids (e.g., Sheldon & Strange, 1982; Flege et al., 1995; Bradlow et al., 1997; 1999; Iverson et al., 2003; Aoyama et al., 2004; Saito & Munro, 2014; Saito & van Poeteren, 2018; Shinohara & Iverson, 2018; Aoyama et al., 2019; Aoyama et al., 2023). 

Here is a brief (and rather coarse) summary of the previous findings:

- English /l ɹ/ are one of the most difficult (if not impossible) sounds for L1 Japanese speakers to acquire in a ''target-like'' manner.

- Because Japanese has only one liquid phoneme /r/, they assimilate English /l/ and /ɹ/ into the Japanese /r/ category, leading to their substantial difficulty in (1) distinguishing the two phonemes and (2) producing them in  a ''target-like'' manner. 

- Most notably, L1 Japanese speakers do not make a good use of the third formant (F3), which is an important cue especially for English /ɹ/.

  - They tend to produce English /ɹ/ with somewhat higher F3 than L1 English speakers.
  
  - In contrast, they seem to be able to produce English /l/ and /ɹ/ with a ''target-like'' use of second formant (F2). 
  
  - Also, they use F2 to distinguish English /l/ and /ɹ/, as opposed to L1 English speakers who distinguish the two phonemes along F3.
  
  - Also also, the first formant (F1) is pretty much comparable between English /l/ and /ɹ/ produced by L1 Japanese and L1 English speakers.

So, I hope you can see that lots of things happen in the spectral analysis! We'll have lots of things to say about these by the end of this two-day workshop. Also, I'm introducing one more variable of interest below.

## Overarching question

There have been quite a few studies investigating L1 Japanese speakers' production of L2 English liquids. To our surprise, however, there are not many studies in which the production data are analysed instrumentally using acoustic and/or articulatory methods. Previous instrumental acoustic studies would include: Flege et al. (1995), Saito and Munro (2014), Saito and van Poeteren (2018), Aoyama et al. (2019), Aoyama et al. (2023). This list is not exhaustive but does cover pretty much the main ones out there. 

These studies commonly utilise **static** analysis in characterising the acoustic profiles of L1 Japanese speakers' production of L2 English liquids. Thanks to these studies, we now understand the general acoustic properties involved in the production. These studies, however, have not managed to explain **how** and **why** L1 Japanese speakers end up producing non-target-like L2 English liquids. 

So, in this workshop, we will explore ways to better explain the mechanism as to why L1 Japanese speakers have substantial difficulty in producing L2 English liquids in a target-like manner based on acoustic data. To rephrase this into a question:

**What is the underlying mechanism of the L1 Japanese speakers' difficulty in producing L2 English liquids in a target-like manner?**

The key to the question would be fine-grained phonetic details. In particular, one aspect that is missing from the previous research is **the effects of vowel contexts**. I will not go into further details, but the vocalic coarticulation can be an important aspect in better understanding the liquid consonants across languages in general. Somewhat surprisingly, on the other hand, all previous research on this Japanese /r/-/l/ problem either (1) consider English /l/ and /ɹ/ production only in one vowel context, or (2) aggregate all the production tokens across different vowel contexts into broader categories of English /l/ and /ɹ/. So, there is a clear gap here, especially between phonetics research and second language speech learning research. And this is what we are going to explore in the next two days using different data analysis methods. 

## Workshop materials

In this workshop, we will be using the materials from one of my previous publications. Data and codes for the analysis are publicly available on the Open Science Framework (OSF) repository, and we will retrieve the data sets from there.

If interested, the article can be accessed here (open access):

Nagamine, T. (2024). Formant dynamics in second language
speech: Japanese speakers’ production of English liquids. *The Journal of the
Acoustical Society of America, 155*(1), 479–495. https://doi.org/10.1121/10.0024351

Here is the URL for the online supplementary materials deposited on the following OSF repository:

https://osf.io/2phx5/

# Statistics basics

Putting aside the (boring) introductory bits, let's start the fun part: data analysis! As mentioned earlier, we will build things up from the static analysis before going into the dynamic analysis. 

I assume that people have varying degrees of experience in statistics. So, before diving straight into the LMEs, let's cover some basics of statistics. 

## Mean and median: the distribution "centre"

The most common parameters in describing a given set of data are **mean** and **median**. Assuming the normal distribution, the most common type of data distribution in statistics, both mean and median represent something like **the midpoint**. But they differ in a small detail:

- **Mean**: the sum of all values divided by the number of values. Sensitive to extreme values.

- **Median**: the middle value when lining up all the data points from the smallest to the greatest. More robust to extreme values than the mean.

The figures below illustrate mean and median values in (1) normal and (2) right skewed distributions. You can see that mean and median values match on the normal distribution (left plot), whereas the mean score is 'attracted' towards the extreme values towards the left (right plot).  

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Load necessary packages
library(tidyverse)
library(ggpubr)

# Set seed for reproducibility
set.seed(123)

# Generate data
data_normal <- tibble(
  distribution = "Normal",
  values = rnorm(1000, mean = 50, sd = 10)  # Symmetric distribution
)

data_skewed <- tibble(
  distribution = "Right-Skewed",
  values = rlnorm(980, meanlog = 6, sdlog = 1.2)  # More extreme right skew
)

# compute summary statistics
stats_normal <- data_normal |> 
  dplyr::summarise(
    mean_value = mean(values),
    median_value = median(values)
  )
  
stats_skewed <- data_skewed |> 
  dplyr::summarise(
    mean_value = mean(values),
    median_value = median(values)
  )

# plot histograms with mean & median
normal_plot <- ggplot(data_normal, aes(x = values)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "skyblue", color = "black", alpha = 0.4) +
  geom_vline(data = stats_normal, aes(xintercept = mean_value, color = "Mean"), linetype = "dashed", linewidth = 1.2) +
  geom_vline(data = stats_normal, aes(xintercept = median_value, color = "Median"), linetype = "solid", linewidth = 1.2) +
  # facet_wrap(~ distribution, scales = "free") +
  scale_color_manual(values = cbPalette) +
  labs(title = "Normal Distribution",
       x = "Value",
       y = "Density",
       color = "Statistic")

skewed_plot <- ggplot(data_skewed, aes(x = values)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "skyblue", color = "black", alpha = 0.4) +
  geom_vline(data = stats_skewed, aes(xintercept = mean_value, color = "Mean"), linetype = "dashed", linewidth = 1.2) +
  geom_vline(data = stats_skewed, aes(xintercept = median_value, color = "Median"), linetype = "solid", linewidth = 1.2) +
  # facet_wrap(~ distribution, scales = "free") +
  scale_color_manual(values = cbPalette) +
  xlim(c(0, 5000)) +
  labs(title = "Skewed Distribution",
       x = "Value",
       y = "Density",
       color = "Statistic")

ggpubr::ggarrange(normal_plot, skewed_plot, common.legend = TRUE, legend = "right")
```


## Standard deviation: the distribution "spread"

Along with the mean and median values, we often encounter **standard deviation (SD)**. SD captures the degree of ''spread'' of a given distribution, or the distance from the mean in the distribution. 

SD tells us the degree of data coverage. For example, one of the distributions shown below has the mean of 50 and the SD of 30 (in the skyblue colour). This indicates that 68% of the data points are covered between -1SD and +1SD: that is, between 50 - 30 = **20** and 50 + 30 = **80**. 

Similarly, 95% of the data points are included within ±2SDs. In the skyblue distribution below, 95% of the data are covered between 50 - 30 $\times$ 2 = **-10** and 50 + 30 $\times$ 2 = **110**. Usually, the larger SD is, the more spread the distribution looks. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Load necessary packages
library(tidyverse)

# Set seed for reproducibility
set.seed(123)

# Generate three normal distributions with the same mean but different SDs
data_sd <- tibble(
  sd_5  = rnorm(1000, mean = 50, sd = 5),
  sd_15 = rnorm(1000, mean = 50, sd = 15),
  sd_30 = rnorm(1000, mean = 50, sd = 30)
) %>%
  pivot_longer(cols = everything(), names_to = "Standard_Deviation", values_to = "Value") %>%
  mutate(Standard_Deviation = factor(Standard_Deviation, 
                                     levels = c("sd_5", "sd_15", "sd_30"),
                                     labels = c("SD = 5", "SD = 15", "SD = 30")))

# Plot density curves for each SD
ggplot(data_sd, aes(x = Value, fill = Standard_Deviation)) +
  geom_density(alpha = 0.5) +  # Transparency for overlapping areas
  geom_vline(xintercept = 50, linetype = "dashed", color = "black", linewidth = 1) +  # Mean line
  geom_vline(xintercept = 0, linetype = "dotted", color = "black", linewidth = 0.5) +  # zero
  scale_fill_manual(values = cbPalette) +  # Custom colors
  labs(title = "Effect of Standard Deviation on Normal Distributions",
       subtitle = "All distributions have the same mean (50) but different standard deviations",
       x = "Value",
       y = "Density",
       fill = "Standard Deviation")

```

## Linear vs non-linear data

Let's take a quick look into what it means by 'linear' and 'non-linear' data. Both are the terms used to explain the relationship between two variables. A linear relationship can be expressed as a straight line, whereas a non-linear relationship requires a curve (or a contour).

The figure below exemplifies a linear and a non-linear relationship between X and Y. Generally, the plot with dots like this is called **'scatter plots'**. Here, the black dots show a linear relationship, where a certain amount of increase in X corresponds to another certain amount of increase in Y, and this X-Y correspondence is consistent throughout the range of X/Y values. This linear relationship can be expressed using a straight (black) line.

The yello dots, on the other hand, show a non-linear relationship between X and Y variables. Although an increase in X corresponds to another increase in Y like in the linear data, the relationship is not consistent, especially towards the end of the x-axis. This relationship cannot be captured with a single line (compare the solid and dotted lines in yellow).

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Load necessary packages
library(tidyverse)

# Simulate a dataset
set.seed(123)

data <- tibble(
  group = rep(c("A", "B"), each = 50),
  x = rnorm(100, mean = 50, sd = 10),
  y_linear = 2 * x + rnorm(100, sd = 5),         # Linear relationship
  y_nonlinear = 0.05 * x^2 + rnorm(100, sd = 10) # Nonlinear relationship
)

# Compute basic statistics
stats <- data |> 
  dplyr::group_by(group) |>
  dplyr::summarise(
    mean_x = mean(x),
    median_x = median(x),
    sd_x = sd(x),
    iqr_x = IQR(x)
  )

# Plot 1: Scatter plot with linear and nonlinear smoothers
ggplot(data, aes(x = x)) +
  geom_point(aes(y = y_linear, color = "Linear"), alpha = 0.7) +
  geom_point(aes(y = y_nonlinear, color = "Nonlinear"), alpha = 0.7) +
  geom_smooth(aes(y = y_linear, color = "Linear"), method = "lm", se = FALSE, linewidth = 1) +
  geom_smooth(aes(y = y_nonlinear, color = "Nonlinear"), method = "loess", se = FALSE, linewidth = 1) +
  geom_smooth(aes(y = y_nonlinear, color = "Nonlinear"), method = "lm", se = FALSE, linewidth = 0.5, linetype = "dashed") +
  labs(title = "Linear vs. Nonlinear Relationships with Smoothers",
       x = "X Variable",
       y = "Y Variable") +
  scale_color_manual(values = cbPalette)
```


## Data visualisation: scatter, box and violin plots

Finally, let's take a quick look at data visualisation methods. I consider that data visualisation is the pivot in data analysis, and a great data analysis is facilitated by effecive data visualiastion. It is also a means to communicate the research findings with the audience. 

Effective data visualisation often combines multiple types of plots. In my usual practice, I often combine three types of plots: **scatter**, **box** and **violin** plots.

1. **Scatter plot**: In the figure below, the dots in skyblue colour represents individual data points. The mean value of the data is expressed as &#9650;.

2. **Box plot**: Superimposed on them is the **box plot**. The box plot consists of a rectangle box, a solid horizontal line in the middle of the box, and 'whiskers' at the top and bottom of the box. Each of them represents key parameters of a distribution:

- The solid horizontal line represents **median**.

- Each end of the box represents **first** and **third quartiles**. Broadly speaking, you could understand that the box covers the middle 50% of the data, which is divided into half by the solid black line. The lower end of the box is the first quartile (Q1) and the upper end is the third quartile (Q3). Right in the middle is the median, which is the second quartile (Q2).

- The length of the whiskers below and above the box is determined such that:

  - Lower end: Q1 - 1.5 $\times$ IQR
  
  - Upper end: Q£ + 1.5 $\times$ IQR

3. **Violin plot**: Finally, the violin plot shows the density of the data distribution. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Load necessary packages
library(tidyverse)

# Set seed for reproducibility
set.seed(123)

# Generate data: Normal distribution with some random noise
data <- tibble(
  values = rnorm(1000, mean = 50, sd = 15)
)

# Calculate mean, median, and IQR for reference
summary_stats <- data |> 
  summarise(
    mean_value = mean(values),
    median_value = median(values),
    q1 = quantile(values, 0.25),
    q3 = quantile(values, 0.75),
    iqr = q3 - q1
  )

# Plot: Show mean, median, and IQR with geom_point, geom_boxplot, and geom_violin
ggplot(data, aes(x = 1, y = values)) +
  # Add jittered points with cbPalette[3] (light blue)
  geom_jitter(alpha = 0.4, color = cbPalette[3], width = 0.3, height = 0.1) +
    # Add violin plot for distribution with cbPalette[6] (blue)
  geom_violin(aes(x = 1, y = values), fill = cbPalette[6], alpha = 0.2) +
  # Add boxplot for IQR with cbPalette[4] (green)
  geom_boxplot(
    aes(x = 1, y = values), 
    width = 0.2, 
    fill = "grey", 
    alpha = 0.7, 
    outlier.shape = NA  # Hide default outliers since we're adding points separately
  ) +
  # Highlight the mean with cbPalette[1] (black)
  geom_point(
    data = summary_stats, 
    aes(x = 1, y = mean_value), 
    color = cbPalette[1], 
    size = 4, 
    shape = 17
  ) +
  # Add lines for Q1 and Q3 (IQR) with cbPalette[7] (orange-red)
  geom_segment(
    data = summary_stats,
    aes(x = 0.8, xend = 1.2, y = q1, yend = q1),
    color = cbPalette[7], size = 1, linetype = "dashed"
  ) +
  geom_segment(
    data = summary_stats,
    aes(x = 0.8, xend = 1.2, y = q3, yend = q3),
    color = cbPalette[7], size = 1, linetype = "dashed"
  ) +
    # Add text labels for mean, median, and IQR (Q1 and Q3)
  geom_text(
    data = summary_stats, 
    aes(x = 1.25, y = median_value, label = paste("Median (Q2) =", round(median_value, 2))), 
    color = cbPalette[1], size = 4, hjust = 0
  ) +
  geom_text(
    data = summary_stats, 
    aes(x = 1.25, y = q1, label = paste("Q1 =", round(q1, 2))),
    color = cbPalette[7], size = 4, hjust = 0
  ) +
  geom_text(
    data = summary_stats, 
    aes(x = 1.25, y = q3, label = paste("Q3 =", round(q3, 2))),
    color = cbPalette[7], size = 4, hjust = 0
  ) +
  labs(
    title = "Mean, Median, and Interquartile Range",
    subtitle = "Highlighted on Jittered, Boxplot, and Violin Plots",
    x = "Distribution",
    y = "Values"
  ) +
  theme(axis.title.x = element_blank(), axis.text.x = element_blank())  # Hide x-axis labels


```



# Static analysis 1: Linear mixed-effect modelling

With the quick review of the statistics basics in mind, let's first take a look at our first **top-down** approach to the formant data. We'll first try a **static** analysis using linear mixed-effect modelling.

# Preliminaries

## Installing/loading packages

Let's first install and load R packages that we are using in the static analysis section. The installation commands have been commented out, but please uncomment them and install the packages if you do not have them on your machine yet.  

```{r warning=FALSE, message=FALSE}
# installing
# install.packages("tidyverse")
# install.packages("lme4")
# install.packages("lme4Test")
# install.packages("emmeans")

# importing
library(tidyverse)
library(lme4)
library(lmerTest)
library(emmeans)
```


A common approach to spectral analysis would be to extract formant frequencies from a single point in time and model them using linear mixed-effect models. Let's try this to see what the analysis offers.

# Importing data set

Let's import the data set. We are using the data set openly available on the Open Science Framework (OSF) repository.

```{r message=FALSE}
# import the csv file "initial.liquid.static.csv" from the "data" directory and save it as "df_mid"
df_mid <- readr::read_csv("data/initial.liquid.static.csv")
```

# Checking data

It's always a good idea to spend some time inspecting the data set. I usually start with checking the column names using ```colnames()```. 

```{r}
# Let's check what columns the data frame contains
colnames(df_mid)
```

According to the code above, the data frame contains the following columns:

- **file**: The name of source audio files. A legacy information from the acoustic analysis.

- **speaker**: Randomly generated speaker IDs consisting of a combination of letters and numbers. 

- **language**: Speaker's first language: either English or Japanese

- **duration**: Duration of the liquid phoneme.

- **segment**: Types of the segment, either L (/l/) or R (/ɹ/)

- **word**: Words that the speakers produced. 

- **f1**, **f2** and **f3**: Formant frequencies in Hz.

- **previous_sound**: The sound occurring before the word-initial liquid consonant. Empty as the words were uttered in isolation. 

- **next_sound**: The sound occurring after the word-initial liquid consonant. In this case, this shows vowel context. 

- **percent**: Proportional time during the liquid consonant. The data set only contains midpoint measurement, so this should be all **50**(%).

- **IsApproximant**: Auditory and visual judgement (based on the spectrogram) as to whether the liquid consonant can be classified as an approximant or not (i.e., an alveolar stop or flap). 

- **IsAcoustic**: Auditory and visual judgement as to the clarity/visibility of the formant structure on the spectrogram. Broadly corresponding to the recording quality.

- **gender**: Speakers' gender. Either male or female.

- **omit**: The column indicating whether acoustic analysis tool omitted the token from the analysis. Irrelevant for the purpose of this workshop. 

- **position**: The syllabic (or word) position of the liquid consonant. All should be **initial**. 

# Data tidy-up

There are quite a few columns in the data set but not all of them are relevant throughout. Let's tidy up the data frame so that the subsequent analysis will be more straightforward. 

## Check data

First, we will inspect a summary of the data according to the columns that we have so far. Although not the most efficient, my approach is to combine ```dplyr::group_by()``` and ```dplyr::summarise()``` functions to summarise the data. Do not forget ```dplyr::ungroup()``` at the end of each operation, too!

```{r}
# Let's check the number of "approximant" tokens
df_mid |> 
  dplyr::group_by(IsApproximant) |> 
  dplyr::summarise() |> 
  dplyr::ungroup()

# Let's check the number of tokens of good recording quality
df_mid |> 
  dplyr::group_by(IsAcoustic) |> 
  dplyr::summarise() |> 
  dplyr::ungroup()

# How about 'omit'?
df_mid |> 
  dplyr::group_by(omit) |> 
  dplyr::summarise() |> 
  dplyr::ungroup()
```

## Omitting irrelavent columns

The codes above show that the data set only contains liquid tokens that are classified as an approximant, deemed as of good recording quality, and included in the acoustic analysis. This means that we can safely remove these columns. Let's do this via ```dplyr::select()``` function. After that, we check the column names again using ```colnames()```, showing that the three columns have been safely removed from the data frame. 

```{r}
# Remove columns 
df_mid <- df_mid |> 
  dplyr::select(-c(IsApproximant, IsAcoustic, omit))
```

### Your turn

Please write a code below to check the updated column names:

```{r}
# Check column names again
# ...
```

## Renaming variables

Later on in this workshop, we will be modelling the formant frequencies as a function of vowel context. In terms of the current data set, this corresponds to the **next_sound** column, but let's rename it into **vowel** to make it more intuitive by adding a new column. 

Also, we will also convert the ARPABET notations into the IPA symbols. 

```{r}
df_mid <- df_mid |> 
  dplyr::mutate(
    vowel = case_when(
      next_sound == "AE1" ~ "/æ/",
      next_sound == "IY1" ~ "/i/",
      next_sound == "UW1" ~ "/u/",
    )
  )
```


## Checking the number of participants, tokens...

Let's also obtain some descriptive statistics here.

```{r}
# number of participants
df_mid |> 
  dplyr::group_by(language) |> 
  dplyr::summarise(n = n_distinct(speaker)) |> 
  dplyr::ungroup()

# number of tokens per segment
df_mid |> 
  dplyr::group_by(segment) |> 
  dplyr::summarise(n = n()) |> 
  dplyr::ungroup()
```

### Your turn

Is there anything else that you would like to know about the data set?

You can start with checking the column names to see what variables are available in the data set. Then, use ```dplyr::group_by()```, ```dplyr::summarise()``` and ```dplyr::ungroup()``` functions to inspect the data.

```{r}
# Check data further
# ...
```


# Data visualisation

Hopefully, you start to become relatively farmiliar with the data set through the checking and tidying-up process above. Now, in order to understand the overall trend of the data, let's try some data visualisation. I heavily rely on the ```ggplot2``` package in the ```tidyverse``` suite. 

## Preparation

Before visualising the data, we need to make sure that the formant frequencies are comparable across speakers. We know that we cannot directly compare raw formant frequencies because they are heavily influenced by the vocal tract characteristics depending on the speaker's gender, sex, height etc. 

To address this, let's normalise the formant frequencies across speakers. There are various normalisation methods, but today we are just using **z-score** normalisation, in which **the mean value** for each speaker is expressed as zero and **the standard deviation** is scaled to one. This can be achieved via ```scale()``` function included in base R. 

Let's create ```f1z```, ```f2z``` and ```f3z``` columns using the ```dplyr::mutate()``` function. When doing this, make sure that you do the scaling for each speaker by specifying ```dplyr::group_by(speaker)```.

```{r}
df_mid <- df_mid |> 
  dplyr::group_by(speaker) |> # tell R to do the following iteration per speaker
  dplyr::mutate(
    f1z = as.numeric(scale(f1)), # scale f1 into z-score
    f2z = as.numeric(scale(f2)), # scale f2 into z-score
    f3z = as.numeric(scale(f3)) # scale f3 into z-score
  ) |> 
  dplyr::ungroup() # don't forget ungrouping
```

Let's check the mean and SD for both raw and normalised formant values: just see F1 for now. Note that the mean z-scores do not seem to look zero, but this is because computers are not very good at dealing with very small numbers (e.g., decimals) and some fluctuations occur in computing the values.

```{r}
# check mean and sd of raw/scaled F1 values for each speaker
df_mid |> 
  dplyr::group_by(speaker) |>
  dplyr::summarise(
    f1_mean = mean(f1),
    f1_sd = sd(f1),
    f1z_mean = mean(f1z),
    f1z_sd = sd(f1z)
  ) |> 
  dplyr::ungroup() 
```

### Your turn

Write a code chunk to inspect the mean and sd of raw/scaled F2 and F3 values for each **speaker** and for **speaker group**. Think how you group the data in ```dplyr::group_by()```.

```{r}
# check mean and sd of raw/scaled F2 values for each speaker group
# ...

# check mean and sd of raw/scaled F3 values for each speaker group
# ...
```

## Visualisation

Let's compare F1 values across participant groups for each segment (/l/ and /ɹ/). For clearer visualisation, the following codes define the range of y-axis between -4 and 4, and some extreme values have been omitted from the plot. 

```{r warning=FALSE}
# F1
df_mid |> 
  ggplot(aes(x = language, y = f1z)) +
  geom_jitter(aes(colour = language), width = 0.3, alpha = 0.4) +
  geom_violin(alpha = 0.4) +
  geom_boxplot(width = 0.4, alpha = 0.4) +
  geom_hline(yintercept = 0, linetype = "dashed", linewidth = 0.5) +
  scale_colour_manual(values = cbPalette) + 
  ylim(c(-4, 4)) +
  facet_grid(segment ~ vowel) +
  labs(x = "L1", y = "F1 (z-normalised)", title = "F1 frequency")

# F2
df_mid |> 
  ggplot(aes(x = language, y = f2z)) +
  geom_jitter(aes(colour = language), width = 0.3, alpha = 0.4) +
  geom_violin(alpha = 0.4) +
  geom_boxplot(width = 0.4, alpha = 0.4) +
  geom_hline(yintercept = 0, linetype = "dashed", linewidth = 0.5) +
  scale_colour_manual(values = cbPalette) + 
  ylim(c(-4, 4)) +
  facet_grid(segment ~ vowel) +
  labs(x = "L1", y = "F2 (z-normalised)", title = "F2 frequency")

# F3
df_mid |> 
  ggplot(aes(x = language, y = f3z)) +
  geom_jitter(aes(colour = language), width = 0.3, alpha = 0.4) +
  geom_violin(alpha = 0.4) +
  geom_boxplot(width = 0.4, alpha = 0.4) +
  geom_hline(yintercept = 0, linetype = "dashed", linewidth = 0.5) +
  scale_colour_manual(values = cbPalette) + 
  ylim(c(-4, 4)) +
  facet_grid(segment ~ vowel) +
  labs(x = "L1", y = "F3 (z-normalised)", title = "F3 frequency")
```

Overall, the data visualisation so far seems to suggest:

1. It seems that vowel contexts may influence the F1 values, such that the /æ/ context yields the overall highest F1 values. The two groups of speakers do not seem to exhibit clear differences along the F1 dimension. 

2. There also seems to be the effect of vowel context, in which F2 is overall highest in the /i/ context. L1 Japanese speakers seem to produce English liquids with higher F2 in the /i/ context and lower F2 in the /u/ context than L1 English speakers. 

3. Whereas F3 seems comparable between the two speaker groups for /l/, L1 Japanese speakers seem to produce English /ɹ/ with a little higher F3 than L1 English speakers, although the magnitude of such between-group differences seem to vary depending on the vowel context. 

### Your turn

Data visualisation is a crucial aspect in data analysis and in open data science: Slight changes in data visualiastion would convey different messages to whoever see the plots. It is always beneficial (and fun!) to play around a little with ```ggplot```. So, let's stop for a moment and have a bit of fun in data visualisation. 

Explore different ways to visualise the F1, F2 and F3 values. For example, you could:

- throw different variables in the ```facet_wrap()``` command

- plot raw F1, F2 and F3 values (i.e., simply typing ```f1``` instead of ```f1z```) to see how the two plots compare

- adjust the ```alpha``` values to see what happens 

- completely change the x/y axis labels

- change the order of ```geom_jitter()```, ```geom_violin``` and ```geom_boxplot()```, and/or comment out some of them to disable

- adjust the y-axis range by adjusting the numerics in ```ylim()```

- etc etc...

```{r}
# try your visualisation
# ...
```


# Statistical analysis

The data visualisation seems to suggest some between-group differences, but it still remains unclear exactly how the two groups differ. Here, let's investigate this further using **linear-mixed effect modelling**. 

Today, we will try to model the formant frequencies (i.e., ```f1z```, ```f2z```, and ```f3z```) as outcome variables as a function of predictor variables, including ```language```, ```vowel``` and the interaction between them (i.e., ```language:vowel```).

We will model the formant frequencies separately for /l/ and /ɹ/ in order to reduce complexity in model output interpretation. This means that for F1 there will be two models (i.e., six models in total for F1, F2 and F3). 

My usual modelling strategy is as follows:

1. I first convert the predictor variables into **factor** variables. At the same time, I check how many levels there are for each predictor variable. Empty levels should be removed at this stage as this will influence the modelling outcomes. 

2. I consider random effect structures. Complex random effects often make the models unable to converge, so I usually spend quite a lot of time adding and removing variables while referring to the summary table. 

3. Once the full model is defined, compare the **full** and **nested** models for the statistical significance check. 

## Preparation 1: Separating the data sets

Let's separate the data sets into two: for /l/ and for /ɹ/.

```{r}
# /l/ model
df_mid_L <- df_mid |> 
  dplyr::filter(segment == "L")

# /ɹ/ model
df_mid_R <- df_mid |> 
  dplyr::filter(segment == "R")
```

## Preparation 2: Converting variables into factor

First, let's convert variable types into **factor**. Here is the code for the /l/ model.

```{r}
# ``language`` variable
df_mid_L$language <- as.factor(df_mid_L$language)
levels(df_mid_L$language)

# ``vowel`` variable
df_mid_L$vowel <- as.factor(df_mid_L$vowel)
levels(df_mid_L$vowel)

# ``speaker`` variable: a random effect
df_mid_L$speaker <- as.factor(df_mid_L$speaker)
levels(df_mid_L$speaker)

# ``word`` variable: a random effect
df_mid_L$word <- as.factor(df_mid_L$word)
levels(df_mid_L$word)

## so far so good, but in case there is any unused level, it can be removed using droplevels()
df_mid_L$word <- droplevels(df_mid_L$word)
```

Similarly, for the /ɹ/ model:

```{r}
# ``language`` variable
df_mid_R$language <- as.factor(df_mid_R$language)
levels(df_mid_L$language)

# ``vowel`` variable
df_mid_R$vowel <- as.factor(df_mid_R$vowel)
levels(df_mid_R$vowel)

# ``speaker`` variable: a random effect
df_mid_R$speaker <- as.factor(df_mid_R$speaker)
levels(df_mid_R$speaker)

# ``word`` variable: a random effect
df_mid_R$word <- as.factor(df_mid_R$word)
levels(df_mid_R$word)

## so far so good, but in case there is any unused level, it can be removed using droplevels()
df_mid_R$word <- droplevels(df_mid_R$word)
```

One important thing to define is the **coding scheme**. Here, I use **treatment coding** because it allows me to understand the model output rather intuitively. 

This means that we need to make sure which level in a given variable serves as a **baseline** level. This can be checked via ```contrasts()``` function as below.

```{r}
# language variable
contrasts(df_mid_L$language)

# vowel variable
contrasts(df_mid_L$vowel)
```

The /ɹ/ data set should have the same coding scheme. These overall suggest that the intercept corresponds to the z-scored formant frequency (along F1, F2 or F3) for L1 English speakers in the /æ/ context. The output table should show a difference from the baseline level. 

## Building models: /l/

Then, let's build a model for F1. We'll begin with the /l/ models.

```{r}
# F1 full model
lm1_L_F1 <- lme4::lmer(f1z ~ 1 + language + vowel + language:vowel + (1 + vowel|speaker) + (1 + language|word), data = df_mid_L, REML = FALSE)
```

We get a **singular fit** warning. This indicates that correlations between random effect variables may not be modelled properly. Let's investigate the model summary to identify the possible causes.

```{r}
summary(lm1_L_F1)$varcor
```

The output above shows that the varying slope/intercept correlation for ```word``` is -1.00. This is usually a red flag indicating some estimation problems in the model. So we'll remove the varying slope for ```word``` depending on the ```language``` variable and rerun the new model below. 

```{r}
# F1 full model
lm2_L_F1 <- lme4::lmer(f1z ~ 1 + language + vowel + language:vowel + (1 + vowel|speaker) + (1|word), data = df_mid_L, REML = FALSE)
```

Now we receive no warning messages, so looking positive. Let's investigate the model output.

```{r}
# random effect correlations
summary(lm2_L_F1)$varcor

# fixed effects
summary(lm2_L_F1)$coefficients
```

Let's spend some time interpreting the fixed effects.

1. **Intercept** shows an estimate of z-normalised F1 values for the baseline level: /l/ for L1 English speakers in the /æ/ condition. This is predicted to be approximately 0.74. 

2. **languageJapanese** shows a difference in F1z values between L1 English (baseline) and L1 Japanese speakers **within the /æ/ context**. This means that, in the /æ/ context, L1 Japanese speakers exhibit the F1 of ca. 0.55 (i.e., 0.74 - 0.19). 

3. **vowel/i/** shows a difference **within L1 English speakers** in F1z values between the /æ/ and /i/ contexts. This means that L1 English speakers produce English /l/ with the z-normalised F1 values of -0.35 in the /i/ context (i.e., 0.74 - 1.09).

4. **vowel/u/** shows a difference, again, within L1 English speakers in F1z values **between the /æ/ and /u/ contexts**. So far, we know that the F1z value for L1 English speakers in the /æ/ context is 0.74: based on this, we can get the F1z value in the /u/ context, which should be approximately -0.65 (i.e., 0.74 - 1.39).

So far, the only occasion when we talked about L1 Japanese speakers was for the **languageJapanese** term. The rest of the two interaction terms get us to obtain F1z values for L1 Japanese speakers in the /i/ and /u/ contexts.

5. In order to obtain the estimated F1z value for L1 Japanese speakers in the /i/ context, we start with the intercept (L1 English speakers in /æ/: **0.74**), then we go onto the /i/ context (still within L1 English speakers: 0.74 **- 1.09**), where we go to L1 Japanese speakers by adding languageJapanese (i.e., 0.74 - 1.09 **- 0.19**). Finally, **languageJapanese:vowel/i/** acts like an adjustment for the interaction term, so that the calculation would be 0.74 - 1.09 - 0.19 + **0.09** = -0.45.

6. Similarly, let's do the same to get estimated F1z values for L1 Japanese speakers in the /u/ context. Starting from the baseline (0.74), then going onto the /u/ context (i.e., 0.74 **- 1.39**), then onto Japanese (i.e., 0.74 - 1.39 **- 0.19**), and finally the **languageJapanese:vowel/u/** adjustment of - 0.01, resulting in 0.74 - 1.39 - 0.19 - **0.01** = -0.85.

Let's compare these estimated values with the raw data. The model estimates can be obtained using the ```emmeans::emmeans()``` function.

```{r}
# raw data
df_mid_L |> 
  dplyr::group_by(language, vowel) |> 
  dplyr::summarise(
    mean_f1z = mean(f1z)
  ) |> 
  dplyr::ungroup()

# estimated marginal means from the model
emmeans::emmeans(lm2_L_F1, ~ vowel | language)
```

The raw data do not always match the estimated values as there are some other factors that are not incorporated in the modelling. But overall, the estimated values are close to the raw values so we should be good to go. 

## Assumption check

Statistical models usually rely on assumptions. Let's check whether the linear model we've just fit meets the **normality** and **constant variance** assumptions.

When the model meets the normality assumption, the **residuals** are usually normally distributed. The residual is the error between an observed data value and a predicted value. Similarly, when the model satisfies the constant variance assumption (i.e., **heteroscedasticity**), the spread of residuals should be constant along the regression line.

The first plot shows the histgram of residuals, and it seems to show that the residuals are normally distributed. The plot in the middle is called a **quantile-quantile (Q-Q) plot**, in which the residuals show a (nearly) linear relationships when satisfying the residual assumption. Finally, the **residual plot** on the right should show that the spread of the residuals is approximately equal across the range of fitted values without any particular linear trend. 

The plots below suggest that our model ```lm2_L_F1``` meets all these assumptions. So far, so good. 

```{r}
par(mfrow = c(1, 3))

# histogram
hist(residuals(lm2_L_F1), col = "skyblue2")

# Q-Q plot
qqnorm(residuals(lm2_L_F1))
qqline(residuals(lm2_L_F1))

# residual plot
plot(fitted(lm2_L_F1), residuals(lm2_L_F1))
```


## Significance testing through likelihood ratio tests (LRTs)

Now that we better understand the model, let's check statistical significance of the fixed effects. A recommended approach is through model comparison via likelihood ratio tests (LRTs) using ```anova()``` function. Model comparison requires two models: a model with the effect of interested included and another (reduced/nested) model that excludes the effect to be tested. 

Note also that, for model comparisons, each model needs to be estimated using **maximal likelihood (ML) estimation**. This corresponds to the ```REML = FALSE``` argument in the model. 

### Interaction between ```language``` and ```vowel```

First, let's test whether the interaction between ```language``` and ```vowel``` is statistically significant. For this, we compare two models: (1) a full model that contains the interaction term and (2) a reduced model without the interaction term. 

```{r}
# F1 full model
lm2_L_F1 <- lme4::lmer(f1z ~ 1 + language + vowel + language:vowel + (1 + vowel|speaker) + (1|word), data = df_mid_L, REML = FALSE)

# F1 nested model -- without interaction
lm3_L_F1 <- lme4::lmer(f1z ~ 1 + language + vowel + (1 + vowel|speaker) + (1|word), data = df_mid_L, REML = FALSE)

# model comparison
anova(lm2_L_F1, lm3_L_F1, test = "Chisq")
```

Model comparison computes a *p* value, showing whether a fuller model improves the degree of model fit at statistically significant level. The output above shows a *p* value above 0.05, meaning that there is little evidence that the two models just compared here differ in the degree of model fit. In other words, the interaction between ```language``` and ```vowel``` does not contribute much to improve the model. 

In this case, it is generally preferred to select more **parsimonious** (i.e., simpler) model because it is more economical in the sense that fewer predictors achieve the same (or even better) degree of model fits. This means that there are now two fixed effects to test: ```language``` and ```vowel```. 

### Main effect of ```language```

Let's check whether ```language``` has an overall effect on the F1z values. 

```{r}
# F1 nested model -- without interaction
lm3_L_F1 <- lme4::lmer(f1z ~ 1 + language + vowel + (1 + vowel|speaker) + (1|word), data = df_mid_L, REML = FALSE)

# F1 nested model -- without language
lm4_L_F1 <- lme4::lmer(f1z ~ 1 + vowel + (1 + vowel|speaker) + (1|word), data = df_mid_L, REML = FALSE)

# model comparison
anova(lm3_L_F1, lm4_L_F1, test = "Chisq")
```

The model comparison output suggests that ```lm3_L_F1``` (i.e., fuller model) has a better model fit at statistically significant level at *p* = 0.03. This means that the effect of ```language``` has an overall effect on the F1z values at statistically significant level.

### Main effect of ```vowel```

How about the effect of ```vowel```? Let's check this here:

```{r}
# F1 nested model -- without interaction
lm3_L_F1 <- lme4::lmer(f1z ~ 1 + language + vowel + (1 + vowel|speaker) + (1|word), data = df_mid_L, REML = FALSE)

# F1 nested model -- without vowel
lm5_L_F1 <- lme4::lmer(f1z ~ 1 + language + (1|speaker) + (1|word), data = df_mid_L, REML = FALSE)

# model comparison
anova(lm3_L_F1, lm5_L_F1, test = "Chisq")
```

The model output still suggests that ```lm3_L_F1``` (i.e., fuller model) shows an improved model fit at statistically significant level of *p* < 0.001.

## Post-hoc analysis

Now that we know that ```language``` and ```vowel``` have a statistically significant effect on F1, let's further investigate where exactly the statistical siginificance is identified. The LRTs tell us which variable has an statistically significant effect, it does not tell us which levels. 

For this, we can use the ```emmeans::emmeans()``` function for pairwise comparison. Although the ```language``` term is fairly obvious given there are only two levels, let's try this anyway to see what the output looks like. 

```{r}
# language effect
emmeans::emmeans(lm3_L_F1, pairwise ~ language, adjust = "tukey")

# vowel effect
emmeans::emmeans(lm3_L_F1, pairwise ~ vowel, adjust = "tukey")
```

For the ```language``` effect, L1 English and L1 Japanese speakers differ very obviously. 

For the ```vowel``` effect, it turns out that statistically siginificant differences are identified between (1) /æ/ and /i/ and (2) /æ/ - /u/, but not between /i/ and /u/. This suggests that it is /æ/ that drives the statistical significance of the ```vowel``` effect, which makes sense as it is the only low/open vowel for which F1 tends to be higher. 

# Summary: F1 for English /l/

- The best fit model:

  - ```f1z ~ 1 + language + vowel + (1 + vowel | speaker) + (1 | word)```

- Both ```language``` and ```vowel``` have an overall effect on the z-normalised F1 values for English /l/. 

  - The ```language``` effect obviously results from the L1 English - L1 Japanese speakers difference.
  
  - The ```vowel``` effect seems to be driven by /æ/ that shows higher F1 than /i/ or /u/.

# Statistics: Your turn!

## Modelling other parameters

So far, we have modelled F1 values for English /l/. How about F2 and F3? How about English /ɹ/? Follow the procedure commented below and complete an analysis of a parameter that you're interested in. 

To reiterate: 

- the data set for English /l/ is ```df_mid_L```

- the data set for English /ɹ/ is ```df_mid_R```

### Check the column names

```{r}
# check the column names
# ...
```

### Convert each variable into factor

```{r}
# convert ``language`` variable into factor variable
# ...

# ``vowel`` variable 
# ...
# ...

# ``speaker`` variable: a random effect
# ...
# ...

# ``word`` variable: a random effect
# ...
# ...

## so far so good, but in case there is any unused level, it can be removed using droplevels()
## try it with the ```word``` variable
# ...
```

### Check the baseline level 

Our modelling is based on the treatment coding scheme, so let's check the baseline level. 

```{r}
# language variable
# ...

# vowel variable
# ...
```

### Building a full model

```{r}
# ...
```

### Assumption check

```{r}
# ...
```

### Significance testing

```{r}
# ...
```


## Advanced: modelling English /l/ and /ɹ/ together

We have decided to build separate models for English /l/ and /ɹ/. But it is also possible to model both together. How would you do it? How do such differences in modelling influence the way you formulate research questions?

# Wrap-up question

Let's wrap up this session by discussing what research questions you could possibly ask based on the static analysis using linear mixed-effect models. Does it fully explain the mechanism underlying the Japanese /r/-/l/ problem? Do you struggle to disentangle something from something else? 

# Session information

```{r}
sessionInfo()
```


